{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import sklearn.model_selection as ms\n",
    "import xgboost as xgb\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in train and test as Pandas DataFrames\n",
    "\"\"\"\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>feat_009</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_248</th>\n",
       "      <th>feat_249</th>\n",
       "      <th>feat_250</th>\n",
       "      <th>feat_251</th>\n",
       "      <th>feat_252</th>\n",
       "      <th>feat_253</th>\n",
       "      <th>feat_254</th>\n",
       "      <th>feat_255</th>\n",
       "      <th>feat_256</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  feat_001  feat_002  \\\n",
       "0  c1ccc(o1)-c1ccc(s1)-c1cnc(-c2scc3[se]ccc23)c2n...       0.0       0.0   \n",
       "1  C1=CC=C(C1)c1cc2ncc3c4[SiH2]C=Cc4ncc3c2c2=C[Si...       1.0       0.0   \n",
       "2  [nH]1c-2c([SiH2]c3cc(-c4scc5C=CCc45)c4nsnc4c-2...       1.0       0.0   \n",
       "3  [nH]1c2-c3occc3Cc2c2c1cc(-c1cccc3=C[SiH2]C=c13...       1.0       0.0   \n",
       "4     c1cnc2c3oc4cc(-c5ncncn5)c5nsnc5c4c3c3cocc3c2c1       0.0       0.0   \n",
       "\n",
       "   feat_003  feat_004  feat_005  feat_006  feat_007  feat_008  feat_009  ...   \\\n",
       "0       0.0       0.0       1.0       0.0       1.0       0.0       0.0  ...    \n",
       "1       0.0       0.0       1.0       0.0       1.0       0.0       0.0  ...    \n",
       "2       0.0       0.0       1.0       1.0       1.0       0.0       0.0  ...    \n",
       "3       0.0       0.0       1.0       1.0       1.0       0.0       0.0  ...    \n",
       "4       0.0       0.0       1.0       0.0       1.0       0.0       0.0  ...    \n",
       "\n",
       "   feat_248  feat_249  feat_250  feat_251  feat_252  feat_253  feat_254  \\\n",
       "0       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       1.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2       1.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "3       1.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "4       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   feat_255  feat_256   gap  \n",
       "0       0.0       0.0  1.19  \n",
       "1       0.0       0.0  1.60  \n",
       "2       0.0       0.0  1.49  \n",
       "3       0.0       0.0  1.36  \n",
       "4       0.0       0.0  1.98  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n",
    "df_test = df_test.drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = df_train.sample(n=100000)\n",
    "# df_train_sample = df_train # only uncomment this line if you want to train on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#store gap values\n",
    "Y_train = df_train_sample.gap.values\n",
    "# #row where testing examples start\n",
    "# test_idx = df_train_sample.shape[0]\n",
    "#delete 'Id' column\n",
    "#delete 'gap' column\n",
    "\n",
    "df_train_sample = df_train_sample.drop(['gap'], axis=1)\n",
    "# code from docs about fingerprinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, actual):\n",
    "    return (np.sum((predictions - actual)**2) / len(predictions))**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example Feature Engineering\n",
    "\n",
    "this calculates the length of each smile string and adds a feature column with those lengths\n",
    "Note: this is NOT a good feature and will result in a lower score!\n",
    "\"\"\"\n",
    "#smiles_len = np.vstack(df_all.smiles.astype(str).apply(lambda x: len(x)))\n",
    "#df_all['smiles_len'] = pd.DataFrame(smiles_len)\n",
    "\n",
    "def convert_fp(smile):\n",
    "    m = Chem.MolFromSmiles(smile)\n",
    "    # http://www.rdkit.org/docs/GettingStartedInPython.html#fingerprinting-and-molecular-similarity\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(m,2,nBits=1024)\n",
    "\n",
    "getBitVector = lambda x: x.ToBitString()\n",
    "getBitVector = np.vectorize(getBitVector)\n",
    "\n",
    "fingerprints = getBitVector(pd.DataFrame(df_train_sample['smiles'].apply(convert_fp))['smiles'].values)\n",
    "df = pd.DataFrame(fingerprints, columns = ['fp'])\n",
    "splitted = df['fp'].apply(lambda x: pd.Series(list((x))))\n",
    "splitted.index = df_train_sample.index\n",
    "df_train_sample = pd.concat([df_train_sample, splitted], axis = 1)\n",
    "df_train_sample.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = df_train_sample.drop(['smiles'], axis=1)\n",
    "\n",
    "# split training set into validation set to check rmse\n",
    "X_train, X_validate, y_train, y_validate = ms.train_test_split(df_train_sample, Y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "LR_pred = LR.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor()\n",
    "RF.fit(X_train, y_train)\n",
    "RF_pred = RF.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPRegressor(hidden_layer_sizes=(700))\n",
    "MLP.fit(X_train, y_train)\n",
    "MLP_pred = MLP.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BayesianRidge(compute_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "ridge_pred = clf.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Regular Gradient Boosted Model -- replaced in favor of the more advanced XGBoost models\n",
    "# GB_params = {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 2,\n",
    "#           'learning_rate': 0.1, 'loss': 'ls', 'max_features': 'sqrt'}\n",
    "# GB = GradientBoostingRegressor(**GB_params)\n",
    "# GB.fit(X_train, y_train)\n",
    "# GB_pred = GB.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB doesn't accept pandas dataframes, so we convert to numpy arrays\n",
    "X_train_BST = X_train.as_matrix()\n",
    "X_validate_BST = X_validate.as_matrix()\n",
    "\n",
    "def find_best_param(param_list, param, min_val, max_val, skip):\n",
    "    # function to find the optimal parameter value between min_val and max_val for XGB model\n",
    "    # param is the parameter to tune\n",
    "    results = []\n",
    "    for tune in np.arange(min_val, max_val + 1, skip):\n",
    "        param_list[param] = tune\n",
    "        BST = xgb.XGBRegressor(**param_list)\n",
    "        BST.fit(X_train_BST, y_train)\n",
    "        BST_pred = BST.predict(X_validate_BST)\n",
    "        RMSE = rmse(BST_pred, y_validate)\n",
    "        results.append((tune, RMSE))\n",
    "        print (\"param: \", tune)\n",
    "        print (\"rmse: \", rmse(BST_pred, y_validate))\n",
    "    smallest = nsmallest(10, results, key = lambda x : x[1])\n",
    "    print (smallest)\n",
    "    return smallest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('param: ', 1000)\n",
      "('rmse: ', 0.17970054114506895)\n",
      "('param: ', 1200)\n",
      "('rmse: ', 0.17908713955374544)\n",
      "('param: ', 1400)\n",
      "('rmse: ', 0.178436774695998)\n",
      "[(1400, 0.178436774695998), (1200, 0.17908713955374544), (1000, 0.17970054114506895)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1400, 0.178436774695998)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## HYPERPARAMETER TUNING\n",
    "# this will take forever unless the number of samples is small (like 1000)\n",
    "BST_params = {'booster': 'dart', 'learning_rate': 0.1}\n",
    "param = 'n_estimators'\n",
    "best = find_best_param(BST_params, param, 1000, 1400, 200)\n",
    "print (best)\n",
    "# we did up to 1400, and 1400 was the best... but for the sake of speed let's use 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('param: ', 3)\n",
      "('rmse: ', 0.1832587202431702)\n",
      "('param: ', 4)\n",
      "('rmse: ', 0.1851966430312995)\n",
      "('param: ', 5)\n",
      "('rmse: ', 0.18236172756277233)\n",
      "('param: ', 6)\n",
      "('rmse: ', 0.1909267809707192)\n",
      "('param: ', 7)\n",
      "('rmse: ', 0.19196166503545173)\n",
      "('param: ', 8)\n",
      "('rmse: ', 0.1994797497396973)\n",
      "('param: ', 9)\n",
      "('rmse: ', 0.1948279825716672)\n",
      "('param: ', 10)\n",
      "('rmse: ', 0.19832220767897982)\n",
      "[(5, 0.18236172756277233), (3, 0.1832587202431702), (4, 0.1851966430312995), (6, 0.1909267809707192), (7, 0.19196166503545173), (9, 0.1948279825716672), (10, 0.19832220767897982), (8, 0.1994797497396973)]\n"
     ]
    }
   ],
   "source": [
    "########## HYPERPARAMETER TUNING\n",
    "param = 'max_depth'\n",
    "best = find_best_param(BST_params, param, 3, 10, 1)\n",
    "print (best)\n",
    "# we tested 3-10, and 5 was best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('param: ', 1)\n",
      "('rmse: ', 0.1778751997787211)\n",
      "('param: ', 2)\n",
      "('rmse: ', 0.17865408471851757)\n",
      "('param: ', 3)\n",
      "('rmse: ', 0.17381468739165046)\n",
      "('param: ', 4)\n",
      "('rmse: ', 0.1800128242478271)\n",
      "('param: ', 5)\n",
      "('rmse: ', 0.17481356800098266)\n",
      "('param: ', 6)\n",
      "('rmse: ', 0.17215753887464105)\n",
      "('param: ', 7)\n",
      "('rmse: ', 0.17008854164047796)\n",
      "('param: ', 8)\n",
      "('rmse: ', 0.17682972952321385)\n",
      "('param: ', 9)\n",
      "('rmse: ', 0.16970147678191272)\n",
      "('param: ', 10)\n",
      "('rmse: ', 0.17118508758238754)\n",
      "[(9, 0.16970147678191272), (7, 0.17008854164047796), (10, 0.17118508758238754), (6, 0.17215753887464105), (3, 0.17381468739165046), (5, 0.17481356800098266), (8, 0.17682972952321385), (1, 0.1778751997787211), (2, 0.17865408471851757), (4, 0.1800128242478271)]\n"
     ]
    }
   ],
   "source": [
    "########## HYPERPARAMETER TUNING\n",
    "param = 'min_child_weight'\n",
    "best = find_best_param(BST_params, param, 1, 10, 1)\n",
    "print (best)\n",
    "# tested 1-10, 9 was the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets BST_params in accordance with the best hyperparameters as previously calculated\n",
    "BST_params = {'booster': 'dart', 'learning_rate': 0.1, 'n_estimators': 1000, 'min_child_weight': 9, 'max_depth': 5}\n",
    "\n",
    "# creates XGB model\n",
    "BST = xgb.XGBRegressor(**BST_params)\n",
    "BST.fit(X_train_BST, y_train)\n",
    "BST_pred = BST.predict(X_validate_BST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to save BST model to a file so we don't have to retrain later\n",
    "import pickle\n",
    "filename = \"bst_model3\"\n",
    "pickle.dump(BST, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08321329209794556\n"
     ]
    }
   ],
   "source": [
    "# print(rmse(LR_pred, y_validate))\n",
    "# print(rmse(RF_pred, y_validate))\n",
    "# print(rmse(MLP_pred, y_validate))\n",
    "# print(rmse(ridge_pred, y_validate))\n",
    "# print(rmse(GB_pred, y_validate))\n",
    "print(rmse(BST_pred, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n"
     ]
    }
   ],
   "source": [
    "# creates prediction vector\n",
    "BST_pred = np.zeros((len(df_test)))\n",
    "for i in np.arange(0, len(df_test) + 1, 10000):\n",
    "    fingerprints = getBitVector(pd.DataFrame(df_test[i:i + 10000]['smiles'].apply(convert_fp))['smiles'].values)\n",
    "    df = pd.DataFrame(fingerprints, columns = ['fp'])\n",
    "    splitted = df['fp'].apply(lambda x: pd.Series(list((x))))\n",
    "    splitted.index = df_test[i:i + len(df)].index\n",
    "    df_test_step = pd.concat([df_test[i:i + len(df)], splitted], axis = 1)\n",
    "    df_test_step.fillna(0, inplace = True)\n",
    "    df_test_step = df_test_step.drop(['smiles'], axis=1)\n",
    "    BST_pred_step = BST.predict(df_test_step.as_matrix())\n",
    "    BST_pred[i:i + len(BST_pred_step)] = BST_pred_step\n",
    "    print (i + 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, predictions):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(predictions):\n",
    "            f.write(str(i+1) + \",\" + str(p) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_file(\"sample1.csv\", LR_pred)\n",
    "write_to_file(\"pred.csv\", BST_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
